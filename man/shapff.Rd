% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/shapff_fit.R
\name{shapff}
\alias{shapff}
\title{Runs shapley forest algorithm}
\usage{
shapff(
  X,
  y,
  Z = NULL,
  shap_model = "full",
  module_membership,
  min_features = 20,
  verbose = 1,
  debug = 2,
  initial = TRUE,
  auto_initial = NULL,
  screen_params = fuzzyforest:::screen_control(min_ntree = 5000),
  select_params = fuzzyforest:::select_control(min_ntree = 5000),
  final_ntree = 5000,
  num_processors = 1,
  parallel = 1,
  nodesize,
  test_features = NULL,
  test_y = NULL,
  nsim = 1,
  final_nsim = 100
)
}
\arguments{
\item{X}{A data.frame. where each column represents a
feature vector.}

\item{y}{Response vector. If performing classification, \code{y} should
be a factor. If performing regression, \code{y}
should be numeric vector.}

\item{Z}{A data.frame of additional features that will bypass
the screening step.}

\item{shap_model}{Binary indicator for \code{shapff} model. If \code{full}, \code{shapff}
runs SHAPley values at both screening and selection step.
If \code{after}, \code{shapff} model runs SHAPley values at the end
of final model and keeps permutation VIMs usage at other steps.
\code{full} is default.}

\item{module_membership}{A vector that specifies the module membership for each
each feature. See \code{shapwff} for possible method.}

\item{min_features}{Defines minimum feature allowed for each module. If \code{debug} is
not \code{-1}, modules below \code{min_features} will only keep non-zero
important features during each Recursive Feature Elimination
iteration}

\item{verbose}{Defines the warning message protocol. If \code{0}, no warning or UI
will be displayed. If \code{1}, warnings and UI progress bar will
be displayed.}

\item{debug}{Sets the debugging procedures. If \code{-1}, all debugging functions
will be bypassed. If \code{0}, debugging at the WGCNA will be bypassed.
Note for \code{shapff}, \code{0} has no effect. If \code{1}, debugging during
Recursive Feature Elimination at both screening and selection step
will be bypassed. If \code{2}, all debugging functions will be ran. Below
are the debugging features. Debugging at WGCNA detects if each module
is below the \code{min_features}. Debugging at RFE will keep only
non zero important feature at each elimination step for modules below
\code{min_features}.}

\item{initial}{Binary indicator to print out initial screening step results (ie the
results from the first Recursive Feature Elimination at the screening
step for each module). If \code{True}, \code{shapff} will pause after RFE
allowing users to select output method for initial screening. If \code{False},
it will bypass all initial screening procedure.}

\item{auto_initial}{Bypass readline prompt for \code{initial}. If \code{1}, \code{initial_screening.csv}
will be saved in directory and stops. If \code{2}, \code{initial_screening.csv}
will be saved in directory and proceeds. If \code{3}, nothing saved and stops.
If \code{4}, nothing saved and proceeds. Default is \code{NULL}. Note if \code{initial},
is set to \code{TRUE}, \code{auto_initial} will automically be set to \code{NULL}.}

\item{screen_params}{Defines the parameter settings for the screening step
of \link[fuzzyforest]{fuzzyforest}.
See \code{\link[fuzzyforest]{screen_control}} for
details. \code{screen_params} is an object of type
\code{screen_control}.}

\item{select_params}{Defines the parameter setting for the selection step
of \link[fuzzyforest]{fuzzyforest}.
See \code{\link[fuzzyforest]{select_control}} for details.
\code{select_params} is an object of type
\code{select_control}.}

\item{final_ntree}{The number of trees grown in the final random forest in
the selection step. This random forest contains all
the surviving features.}

\item{num_processors}{Number of processors used to run random forests.}

\item{parallel}{Type of parellization to be used. \code{1} if
\code{\link[doParallel]{doParallel}}. \code{2} if
\code{\link[doSNOW]{doSNOW}}. \code{1} is the default.}

\item{nodesize}{Minimum terminal nodesize. 1 if classification.
5 if regression.  If the sample size is very large,
the trees will be grown extremely deep.
This may lead to issues with memory usage and may
lead to significant increases in the time it takes
the algorithm to run. In this case,
it may be useful to increase \code{nodesize}.}

\item{test_features}{A data.frame containing features from a test set.
The data.frame should contain the features in both
X and Z. Used during final Random Forest call (after
screening and selection step).}

\item{test_y}{The responses for the test set. Used during final Random
Forest call (after screening and selection step).}

\item{nsim}{Number of Monte Carlo repetitions for estimating SHAP
values in the screening step. Default is \code{1}. Increasing
\code{nsim} leads to more accurate results, but at the cost
of computational cost.}

\item{final_nsim}{Number of Monte Carlo repetitions for estimating SHAP
values in the selection step. Default is \code{1}. \code{final_nsim}
should be as large as feasibly possible.}
}
\value{
Returns an object of type \code{shapley_forest}, which is a list containing the essential
output of shapley forests, including a data.frame of selected features and the random forest
model fitted using those features. See \code{shapley_forest} for more details.
}
\description{
Runs shapley forest algorithm for feature importance through
the use of SHAPley values.
}
\examples{
library(WGCNA)
library(shapleyforest)

# for generating example dataset
library(mvtnorm)

# Note: shapff requires an already partitioned dataset.

sim_mod <- function(n, p, corr) {
sigma <- matrix(corr, nrow = p, ncol = p)
diag(sigma) <- 1
X <- rmvnorm(n, sigma = sigma)
return(X)
}

# parameters
n <- 100
p <- 1000
mtry_factor <- 1
keep_fraction <- 0.1
drop_fraction <- 0.25

corr <- 0.8

number_of_groups <- 10
number_of_mods <- number_of_groups - 1
p_per_group <- p/number_of_groups
vim_list <- c(1:3, 901:903)
vim_interest <- c(1:4, 901:904)
beta_list <- rep(c(5, 5, 2), 2)

all_modules <- lapply(1:number_of_mods, function(j) sim_mod(n, p_per_group, corr))
all_modules[[number_of_groups]] <- matrix(rnorm(p_per_group * n), nrow = n, ncol = p_per_group)
X <- do.call(cbind, all_modules)
beta <- rep(0, p_per_group * (number_of_mods + 1))
beta[vim_list] <- beta_list

group_names <- paste0("Group_", seq_len(1000 / 100))
groups <- rep(group_names, each = 100)

y <- X \%*\% beta + rnorm(n, sd = 0.1)
y <- as.numeric(y)

X <- as.data.frame(X)
names(X) <- paste("V", 1:p, sep = "")

screen_params <- screen_control(drop_fraction = drop_fraction, keep_fraction = keep_fraction, 
                               mtry_factor = mtry_factor)
select_params <- select_control(number_selected = 10, drop_fraction = drop_fraction, 
                               mtry_factor = mtry_factor)

fit <- shapff(X, y, shap_model= "full",
             module_membership = groups,
             select_params = select_params, 
             screen_params = screen_params, 
             auto_initial = 2, num_processors = 1, 
             nodesize = 1, debug = 1, verbose = 0, 
             initial = TRUE)

# print Surviving Final SHAP Values
fit$final_SHAP

# print Initial Screening Output
initial_screen

# Importance Plot
plot_importance(fit)

# Decision Plot
plot_decisions(fit)

## Prediction
# Generates new dataset
# Generate new simulated modules
new_modules <- lapply(1:number_of_mods, function(j) sim_mod(n, p_per_group, corr))
new_modules[[number_of_groups]] <- matrix(rnorm(p_per_group * n), nrow = n, ncol = p_per_group)

new_X <- do.call(cbind, new_modules)
new_X <- as.data.frame(new_X)
names(new_X) <- paste("V", 1:p, sep = "")

# Predict
predictions = predict(fit, new_X)
}
\references{
Daniel Conn, Tuck Ngun, Christina M. Ramirez (2015). Fuzzy Forests: a New
WGCNA Based Random Forest Algorithm for Correlated, High-Dimensional Data,
Journal of Statistical Software, Manuscript in progress.

Leo Breiman (2001). Random Forests. Machine Learning, 45(1), 5-32.

Lundberg, S. M., & Lee, S. I. (2017). A unified approach to interpreting model
predictions. Advances in neural information processing systems, 30.
}
